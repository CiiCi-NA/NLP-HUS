{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113e0ca8",
   "metadata": {},
   "source": [
    "# POS Tagging (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4ff190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "# Config \n",
    "DATA_DIR = \"data/UD_English-EWT\"   \n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"en_ewt-ud-train.conllu\")\n",
    "DEV_FILE   = os.path.join(DATA_DIR, \"en_ewt-ud-dev.conllu\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b8048",
   "metadata": {},
   "source": [
    "# Hàm đọc file .conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ec44cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/UD_English-EWT\\\\en_ewt-ud-train.conllu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m train_sents \u001b[38;5;241m=\u001b[39m \u001b[43mload_conllu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m dev_sents   \u001b[38;5;241m=\u001b[39m load_conllu(DEV_FILE)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_sents))\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mload_conllu\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mĐọc file conllu, trả về danh sách câu; mỗi câu là danh sách (word, upos)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mIgnores multiword tokens / comments; uses column 2 (FORM) and 4 (UPOS).\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     sent \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\CiiCi\\anaconda3\\envs\\tf_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/UD_English-EWT\\\\en_ewt-ud-train.conllu'"
     ]
    }
   ],
   "source": [
    "def load_conllu(file_path: str) -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Đọc file conllu, trả về danh sách câu; mỗi câu là danh sách (word, upos)\n",
    "    Ignores multiword tokens / comments; uses column 2 (FORM) and 4 (UPOS).\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sent = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sent:\n",
    "                    sentences.append(sent)\n",
    "                    sent = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            # skip multiword or empty id lines (like \"1-2\")\n",
    "            token_id = parts[0]\n",
    "            if \"-\" in token_id or \".\" in token_id:\n",
    "                continue\n",
    "            form = parts[1]\n",
    "            upos = parts[3]\n",
    "            sent.append((form, upos))\n",
    "        # last sentence\n",
    "        if sent:\n",
    "            sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "# Load data\n",
    "train_sents = load_conllu(TRAIN_FILE)\n",
    "dev_sents   = load_conllu(DEV_FILE)\n",
    "\n",
    "print(\"Train sentences:\", len(train_sents))\n",
    "print(\"Dev sentences:  \", len(dev_sents))\n",
    "# print example\n",
    "print(train_sents[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2a404",
   "metadata": {},
   "source": [
    "# Xây dựng vocab cho từ & tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801af379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "def build_vocabs(sentences):\n",
    "    word_counter = Counter()\n",
    "    tag_set = set()\n",
    "    for sent in sentences:\n",
    "        for w, t in sent:\n",
    "            word_counter[w] += 1\n",
    "            tag_set.add(t)\n",
    "    # create word_to_ix with PAD at 0, UNK at 1\n",
    "    word_to_ix = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for i, (w, _) in enumerate(word_counter.most_common(), start=2):\n",
    "        word_to_ix[w] = i\n",
    "    tag_to_ix = {PAD_TOKEN: 0}  # pad label index = 0, we'll ignore it in loss\n",
    "    for i, tag in enumerate(sorted(tag_set), start=1):\n",
    "        tag_to_ix[tag] = i\n",
    "    return word_to_ix, tag_to_ix\n",
    "\n",
    "word_to_ix, tag_to_ix = build_vocabs(train_sents)\n",
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "\n",
    "print(\"Vocab size (words):\", len(word_to_ix))\n",
    "print(\"Num tags:\", len(tag_to_ix))\n",
    "print(\"Some tags:\", list(tag_to_ix.items())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477fc85",
   "metadata": {},
   "source": [
    "# Dataset và collate_fn (pad per-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc120b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(Dataset):\n",
    "    def __init__(self, sentences, word_to_ix, tag_to_ix):\n",
    "        self.sentences = sentences\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.sentences[idx]\n",
    "        words = [w for w,t in sent]\n",
    "        tags  = [t for w,t in sent]\n",
    "        # convert to indices\n",
    "        w_idxs = [ self.word_to_ix.get(w, self.word_to_ix[UNK_TOKEN]) for w in words ]\n",
    "        t_idxs = [ self.tag_to_ix[t] for t in tags ]\n",
    "        return torch.tensor(w_idxs, dtype=torch.long), torch.tensor(t_idxs, dtype=torch.long)\n",
    "\n",
    "# collate_fn: pads sequences in a batch\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (word_tensor, tag_tensor) variable lengths\n",
    "    returns: padded_word_tensor (B, L), padded_tag_tensor (B, L), lengths list\n",
    "    \"\"\"\n",
    "    words, tags = zip(*batch)\n",
    "    lengths = torch.tensor([w.size(0) for w in words], dtype=torch.long)\n",
    "    words_padded = pad_sequence(words, batch_first=True, padding_value=word_to_ix[PAD_TOKEN])\n",
    "    tags_padded  = pad_sequence(tags,  batch_first=True, padding_value=tag_to_ix[PAD_TOKEN])\n",
    "    return words_padded, tags_padded, lengths\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = POSDataset(train_sents, word_to_ix, tag_to_ix)\n",
    "dev_dataset   = POSDataset(dev_sents,   word_to_ix, tag_to_ix)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# quick check\n",
    "for xb, yb, lengths in train_loader:\n",
    "    print(\"batch shapes:\", xb.shape, yb.shape, \"lengths:\", lengths[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269b2c3",
   "metadata": {},
   "source": [
    "# Mô hình RNN (Embedding + RNN + Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags, n_layers=1, bidirectional=False, dropout=0.0):\n",
    "        super(SimpleRNNTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=word_to_ix[PAD_TOKEN])\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=n_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          nonlinearity='tanh',\n",
    "                          dropout=dropout if n_layers>1 else 0.0)\n",
    "        self.bidirectional = bidirectional\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.classifier = nn.Linear(rnn_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x: (B, L)\n",
    "        emb = self.embedding(x)  # (B, L, E)\n",
    "        # RNN (we are NOT packing here — packing optional)\n",
    "        rnn_out, _ = self.rnn(emb)  # (B, L, H*directions)\n",
    "        logits = self.classifier(rnn_out)  # (B, L, num_tags)\n",
    "        return logits\n",
    "\n",
    "# Instantiate\n",
    "vocab_size = len(word_to_ix)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_tags = len(tag_to_ix)\n",
    "\n",
    "model = SimpleRNNTagger(vocab_size=vocab_size, embedding_dim=embedding_dim,\n",
    "                        hidden_dim=hidden_dim, num_tags=num_tags,\n",
    "                        n_layers=1, bidirectional=False, dropout=0.1)\n",
    "model.to(DEVICE)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3cd86",
   "metadata": {},
   "source": [
    "# Loss, optimizer, helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss: ignore_index = tag_to_ix[PAD_TOKEN] so pad tokens not counted\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag_to_ix[PAD_TOKEN])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def compute_accuracy(preds, labels, ignore_index=tag_to_ix[PAD_TOKEN]):\n",
    "    \"\"\"\n",
    "    preds: (B, L) long, labels: (B, L)\n",
    "    returns: token-level accuracy ignoring PAD\n",
    "    \"\"\"\n",
    "    mask = (labels != ignore_index)\n",
    "    correct = (preds == labels) & mask\n",
    "    total = mask.sum().item()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return correct.sum().item() / total\n",
    "\n",
    "# For convenience: function to convert indices -> tags\n",
    "def idxs_to_tags(idxs_batch):\n",
    "    # idxs_batch: 1D or 2D tensor\n",
    "    if isinstance(idxs_batch, torch.Tensor):\n",
    "        idxs = idxs_batch.cpu().numpy()\n",
    "    else:\n",
    "        idxs = np.array(idxs_batch)\n",
    "    if idxs.ndim == 1:\n",
    "        return [ix_to_tag.get(int(i), \"UNK\") for i in idxs]\n",
    "    else:\n",
    "        return [[ix_to_tag.get(int(i), \"UNK\") for i in row] for row in idxs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689e249",
   "metadata": {},
   "source": [
    "# Vòng huấn luyện & đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, lengths in data_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            logits = model(xb)  # (B, L, num_tags)\n",
    "            # reshape for loss: (B*L, num_tags), labels (B*L)\n",
    "            B, L, C = logits.shape\n",
    "            logits_flat = logits.view(-1, C)\n",
    "            labels_flat = yb.view(-1)\n",
    "            loss = criterion(logits_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            acc = compute_accuracy(preds, yb)\n",
    "            total_acc += acc\n",
    "            total_batches += 1\n",
    "    avg_loss = total_loss / (total_batches if total_batches>0 else 1)\n",
    "    avg_acc = total_acc / (total_batches if total_batches>0 else 1)\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 10\n",
    "best_dev_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb, lengths in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        B, L, C = logits.shape\n",
    "        logits_flat = logits.view(-1, C)\n",
    "        labels_flat = yb.view(-1)\n",
    "        loss = criterion(logits_flat, labels_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    dev_loss, dev_acc = evaluate(model, dev_loader)\n",
    "    print(f\"Epoch {epoch} | Train loss: {avg_train_loss:.4f} | Dev loss: {dev_loss:.4f} | Dev acc: {dev_acc:.4f}\")\n",
    "    # save best\n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_dev_acc = dev_acc\n",
    "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "        print(\" -> New best dev acc:\", best_dev_acc)\n",
    "# load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "print(\"Training finished. Best dev acc:\", best_dev_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff57f6d",
   "metadata": {},
   "source": [
    "# Hàm predict_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    # basic whitespace tokenizer; you can replace with better tokenizer\n",
    "    return text.strip().split()\n",
    "\n",
    "def predict_sentence(model, sentence: str, word_to_ix, idx_to_tag, device=DEVICE, max_len=None):\n",
    "    model.eval()\n",
    "    tokens = simple_tokenize(sentence)\n",
    "    idxs = [ word_to_ix.get(w, word_to_ix[UNK_TOKEN]) for w in tokens ]\n",
    "    tensor = torch.tensor(idxs, dtype=torch.long).unsqueeze(0)  # (1, L)\n",
    "    tensor = tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)  # (1, L, num_tags)\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze(0)  # (L,)\n",
    "    pred_tags = [ idx_to_tag.get(int(i), \"UNK\") for i in preds.cpu().numpy() ]\n",
    "    return list(zip(tokens, pred_tags))\n",
    "\n",
    "# Example predictions\n",
    "examples = [\n",
    "    \"I love NLP\",\n",
    "    \"She is reading the book\",\n",
    "    \"The quick brown fox jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(s, \"->\", predict_sentence(model, s, word_to_ix, ix_to_tag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5fd6b",
   "metadata": {},
   "source": [
    "# In báo cáo chi tiết: accuracy, vài ví dụ dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dev_loss, dev_acc = evaluate(model, dev_loader)\n",
    "print(\"Final dev loss:\", dev_loss, \"dev acc:\", dev_acc)\n",
    "\n",
    "# Show predictions on first 10 dev sentences\n",
    "for i in range(10):\n",
    "    sent = dev_sents[i]\n",
    "    words = \" \".join(w for w,t in sent)\n",
    "    preds = predict_sentence(model, words, word_to_ix, ix_to_tag)\n",
    "    gold = [t for w,t in sent]\n",
    "    print(\"SENT:\", words)\n",
    "    print(\"PRED:\", preds)\n",
    "    print(\"GOLD:\", gold)\n",
    "    print(\"-\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
